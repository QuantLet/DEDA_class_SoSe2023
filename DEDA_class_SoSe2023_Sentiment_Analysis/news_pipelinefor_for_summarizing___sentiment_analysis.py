# -*- coding: utf-8 -*-
"""News_Pipelinefor_for_Summarizing_&_Sentiment-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AlJdJrLycT1Btm1AZVRex5jWNWilKMef

# 1. Install and Import Baseline Dependencies
"""

!pip install transformers
!pip install sentencepiece

from transformers import PegasusForConditionalGeneration, PegasusTokenizer
from bs4 import BeautifulSoup
import requests
import re
from transformers import pipeline
import csv

"""# 2. Setup Summarization Model"""

model_name = "human-centered-summarization/financial-summarization-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

"""# 3. Building a News and Sentiment Pipeline"""

monitored_tickers = ['MBG']

"""## 3.1 Search for Stock News using Google and Finanzen.net"""

def search_for_stock_news_urls(ticker):
    search_url = "https://www.google.com/search?q=finanzen.net+{}&tbm=nws".format(ticker)
    #search_url = "https://www.google.com/search?q=yahoo+finance+{}&tbm=nws".format(ticker)
    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36' }
    r = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(r.text, 'html.parser')
    #atags = soup.find_all('a')
    atags = soup.find_all('a', href=True)
    hrefs = [link['href'] for link in atags]
    return hrefs

raw_urls = {ticker:search_for_stock_news_urls(ticker) for ticker in monitored_tickers}

raw_urls

"""## 3.2 Strip out unwanted URLs"""

exclude_list = ['maps', 'policies', 'preferences', 'accounts', 'support']

def strip_unwanted_urls(urls, exclude_list):
    val = []
    for url in urls: 
        if 'https://' in url and not any(exclude_word in url for exclude_word in exclude_list):
            res = re.findall(r'(https?://\S+)', url)[0].split('&')[0]
            val.append(res)
    return list(set(val))

cleaned_urls = {ticker:strip_unwanted_urls(raw_urls[ticker], exclude_list) for ticker in monitored_tickers}

cleaned_urls

"""## 3.3 Search and Scrape Cleaned URLs"""

def scrape_and_process(URLs):
    ARTICLES = []
    for url in URLs: 
        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = [paragraph.text for paragraph in paragraphs]
        words = ' '.join(text).split(' ')[:350]
        ARTICLE = ' '.join(words)
        ARTICLES.append(ARTICLE)
    return ARTICLES

articles = {ticker:scrape_and_process(cleaned_urls[ticker]) for ticker in monitored_tickers}

articles

"""## 3.4 Summarise all Articles"""

def summarize(articles):
    summaries = []
    for article in articles:
        input_ids = tokenizer.encode(article,truncation=True, padding="longest", return_tensors='pt')
        #input_ids = tokenizer.encode_plus(article, add_special_tokens=True, max_length=512, truncation=True, padding="max_length")
        output = model.generate(input_ids, max_length=55, num_beams=5, early_stopping=True)
        summary = tokenizer.decode(output[0], skip_special_tokens=True)
        summaries.append(summary)
    return summaries

summaries = {ticker:summarize(articles[ticker]) for ticker in monitored_tickers}

summaries

"""# 4. Adding Sentiment Analysis"""

sentiment = pipeline('sentiment-analysis')
scores = {ticker:sentiment(summaries[ticker]) for ticker in monitored_tickers}

scores

"""# 5. Exporting Results to CSV"""

def create_output_array(summaries, scores, urls):
    output = []
    for ticker in monitored_tickers:
        for counter in range(len(summaries[ticker])):
            output_this = [
                ticker,
                summaries[ticker][counter],
                scores[ticker][counter]['label'],
                scores[ticker][counter]['score'],
                urls[ticker][counter]
            ]
            output.append(output_this)
    return output

final_output = create_output_array(summaries, scores, cleaned_urls)
final_output.insert(0, ['Ticker', 'Summary', 'Label', 'Confidence', 'URL'])

"""
Just for checking & trouble shooting
final_output
"""
with open('output.csv', mode='w', newline='') as f:
    csv_writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    csv_writer.writerows(final_output)