import gensim
from gensim.models import CoherenceModel
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
import matplotlib.pyplot as plt
import pandas as pd
import csv
import numpy as np
import os

class LDA:
    '''
    A custom LDA interface designed to carry out a grid search, find the best model and vizualize it. 
    Grid search carried out per coherence instead of perplexity, as optimizing for the latter may not lead to "human interpretable topics."
    
    Args:
    
        corpus: A bag of words corpus (Already generated by CorpusMaker)
        dictionary: A gensim dictionary (Already generated by CorpusMaker)
        texts: All tokens (Already generated by CorpusMaker)
        
    How to use:
        
        MSc_LDA = LDA(corpus, dictionary, texts) <-- initializes the class
        m = MSc_LDA.simple_fit() <-- fits one model with custom specs
        MSc_LDA.grid_search(n_topics, alphas, betas) <-- conducts grid search
        MSc_LDA.lineplot_scores() <-- plots coherence scores from grid search rounds
        m = MSc_LDA.build_best_model() <-- fits best model
        MSc_LDA.viz() <-- vizualizes best/simple model
        
    '''
    
    def __init__ (self, corpus, dictionary, texts):
        
        # Below the usual def __init__ items: Those selected by the user
        self.corpus = corpus
        self.dictionary = dictionary
        self.texts = texts
        
        # Simple fit model item, set to None at initialization
        self.simple_model = None
        
        # Below the grid search items, that will get overwritten per search iteration
        # Set to none because it gets selected by the algorithm
        self.best_params = None
        
        # Since coherence score ranges from 0 to 1, we should initialize the best score as as anything below 0
        self.best_score = -1
        
        # Set to none due to reason mentioned
        self.best_model = None
        
        
    def get_coherence_score(self, n, alpha, beta):
        
        '''
        Calculates the coherence score per model using gensim's CoherenceModel.
        
        Args:
            n: Number of topics in one LDA model (per iteration of search)
            alpha: Document-Topic Density
            beta: Topic-Word Density
            
        Returns:
            Coherence score, fitted LDA model.
            
        '''
        
        m = gensim.models.LdaModel(corpus = self.corpus,
                                   id2word = self.dictionary,
                                   num_topics = n,
                                   random_state = 66,  # Custom random state used in our project
                                   update_every = 1,
                                   chunksize = 100,
                                   passes = 10,
                                   alpha = alpha, # Alpha grabbed from function arguments
                                   per_word_topics = True,
                                   eta = beta # beta grabbed from function arguments
        )
        
        cm = CoherenceModel(model = m,
                            texts = self.texts,
                            corpus = self.corpus,
                            dictionary = self.dictionary,
                            coherence = 'c_v'
                           )
        
        # Returns the variables defined above
        # This comes in handy when fitting the best model after grid search
        return cm.get_coherence(), m
        
    def simple_fit(self, n_top, alpha_val, beta_val):
        '''
        Fits a "simple" LDA model, without any grid search.
        
        Args:
            n_top: The number of topics 
            alpha_val: Document-Topic Density
            beta_val: Topic-Word Density.
        
        Returns:
            Fitted LDA model.
            
        '''
        
        # Set up LDA model
        model = gensim.models.LdaModel(corpus = self.corpus,
                                   id2word = self.dictionary,
                                   num_topics = n_top,
                                   random_state = 66,  # Custom random state used in our project
                                   update_every = 1,
                                   chunksize = 100,
                                   passes = 10,
                                   alpha = alpha_val, 
                                   per_word_topics = True,
                                   eta = beta_val) 
        
        
        # Get coherence score
        coh_model = CoherenceModel(model = model, texts = self.texts, corpus = self.corpus, dictionary = self.dictionary, coherence = 'c_v')
        coherence = coh_model.get_coherence()
        
        # Get perplexity
        perplexity = model.log_perplexity(self.corpus)
        
        print('\n')
        print(f'Coherence Score is: {coherence}')
        print(f'Perplexity Score is: {perplexity}')
        print('\n')
        print('See the topics:')
        topics = model.print_topics()
        for topic in topics:
            print(topic)
            
        # Save as CSV via pandas
        simple_topics_df = pd.DataFrame(topics, columns = ['Topic N', 'Words'])
        
        # We specify saving path and make sure to create it if it does not exist
        if not os.path.exists('Topics_CSVs'):
            os.makedirs('Topics_CSVs')
        
        simple_topics_df.to_csv(os.path.join('Topics_CSVs', 'Topics_from_simple_model.csv'), index = False)
        
        # We specify saving path and make sure to create it if it does not exist
        if not os.path.exists('LDAModels_Gensim'):
            os.makedirs('LDAModels_Gensim')
        
        
        model.save(os.path.join('LDAModels_Gensim', 'MSc_LDA_simple.gensim'))
        
        # Add as self attribute to be used in viz later
        self.simple_model = model
        
        return model
    
    def grid_search(self, n_topics, alphas, betas, verbose = False):
        
        '''
        Performs grid search, finding optimal LDA parameters
        
        Args:
            n_topics: list of possible topic number (can use list(range( ,)))
            alphas: list of alpha values (can use np.arange( , , ).tolist())
            betas: list of beta values (can use np.arange( , , ).tolist())
            verbose: If set to true, will give information about the number of topics, alpha and beta values and their coherence score per iteration

        Returns:
            self.scores: Coherence and perplexity scores for all LDA models tried out in grid search. 
        '''
        # Empty container for scores
        self.scores = []
        # Begin loop
        for n in n_topics:
            for alpha in alphas:
                for beta in betas:
                    
                    # Get coherence score and model
                    coherence_score, model = self.get_coherence_score(n, alpha, beta)
                    # Get perplexity score from model
                    perplexity_score = model.log_perplexity(self.corpus)
                    
                    # Append into empty container for scores
                    self.scores.append({'n_topics': n,
                                        'alpha': alpha,
                                        'beta': beta,
                                        'coherence_score': coherence_score,
                                        'perplexity_score': perplexity_score,
                       })
                    
                    # The if statement will always be valid in the first iteration
                    # but it will give the best score at the end of the run
                    
                    if coherence_score > self.best_score:
                        self.best_score = coherence_score
                        self.best_params = (n, alpha, beta)
                        self.best_model = model
                    
                    # This gives a very long print output in case of large grid search
                    # so it is only activated if verbose = True
                    if verbose:
                        print(f'Number of topics: {n}; alpha: {alpha}; beta: {beta}; Achieved coherence score: {coherence_score}')
                        
        scores_df = pd.DataFrame(self.scores)
        
        # Same statement as above for saving directory
        if not os.path.exists('Topics_CSVs'):
            os.makedirs('Topics_CSVs')
        
        scores_df.to_csv(os.path.join('Topics_CSVs', 'scores_from_search'), index = False)
        
        
    def lineplot_scores(self):
        '''
        Constructs line plot with number of topics in LDA model on X axis and respective coherence and peplexity scores on Y axis.
        '''
  
        # Plot configuration 
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['figure.figsize'] = [12, 6.75]

        # Use list comprehension to extract the variables we need for plotting 
        topics = [n_topic['n_topics'] for n_topic in self.scores]
        coherence_scores = [score['coherence_score'] for score in self.scores]
        perplexity_scores = [score['perplexity_score'] for score in self.scores]
            
        # Initialize subplots
        fig1, ax1 = plt.subplots()
            
        ax1.set_xlabel('Number of Topics')
        ax1.set_ylabel('Coherence Score')
        ax1.plot(topics, coherence_scores)
        fig1.tight_layout()
        if not os.path.exists('Plots'):
            os.makedirs('Plots')
        plt.savefig(os.path.join('Plots', 'Coherence_Scores.png'), dpi = 300, transparent = True)
        plt.show()
        plt.close()
            
        fig2, ax2 = plt.subplots()       
        ax2.set_xlabel('Number of Topics')
        ax2.set_ylabel('Perplexity')
        ax2.plot(topics, perplexity_scores)
            
        fig2.tight_layout()
        if not os.path.exists('Plots'):
            os.makedirs('Plots')
        plt.savefig(os.path.join('Plots', 'Perplexity_Scores.png'), dpi = 300, transparent = True)        
        plt.show()
        plt.close()
        
        
        # NOTE:
        # This plotting may not be the best option given that we have many variables.
        # We could consider other options. 
        
    
    def build_best_model(self):
        
        '''
        Fits the best model found during the grid search.
        
        Returns:
            The LDA model.
        '''
        
        # A neat line of if statetement included as a flex
        if self.best_params:
            n, alpha, beta = self.best_params
            
            # We do not need the coherence score so leave first blank
            _, model = self.get_coherence_score(n, alpha, beta)
            
            # Save the model to be able to load it via Gensim later
            if not os.path.exists('LDAModels_Gensim'):
                os.makedirs('LDAModels_Gensim')
                    
            model.save(os.path.join('LDAModels_Gensim', 'Grid_Best_MSc_LDA.gensim'))
            
            
            topics = model.print_topics()
            for t in topics:
                print(t)
                
            # Further save the topics as a csv
            if not os.path.exists('Topics_CSVs'):
                os.makedirs('Topics_CSVs')

            
            with open(os.path.join('Topics_CSVs', 'best_topics.csv'), 'w', newline = '') as f:
                writer = csv.writer(f)
                writer.writerow(['Topic N', 'Keywords'])
                for t in topics:
                    writer.writerow(t)
                    
            return model
         
        else:
            raise Exception('No parameters found for the best model. Make sure you have run the grid search already.')

    
    def viz(self, model_type = 'best'):
        
        '''
        Visualizes the optimal LDA model found by gridsearch using pyLDAvis
        
        Args:
            model_type: Specify whether we want the visualization for the model trained through simple_fit method or through the build_best_model (via grid search). Default: best
        
        Returns:
            The visualizations
        '''
        if model_type == 'best':
            model = self.best_model
            if model is None:
                raise Exception('No best model found. Run grid_search first.')
        
        elif model_type == 'simple':
            model = self.simple_model
            if model is None:
                raise Exception('No simple model found. Run simple_fit() first.')
        
        else:
            raise ValueError('Model type not valid. Please select either "best" or "simple".')
        
        # Visualize using pyLDAvis
        pyLDAvis.enable_notebook()
        viz = gensimvis.prepare(model, self.corpus, self.dictionary)
        
        
        # Save the visualization as HTML
        plot_directory = 'Plots'
        if not os.path.exists(plot_directory):
            os.makedirs(plot_directory)
            
        html_path = os.path.join(plot_directory, f'lda_{model_type}_viz.html')
        pyLDAvis.save_html(viz, html_path)
        
        return viz
    
                        